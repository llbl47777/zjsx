# LLM Scheduler Pro - Docker Compose 配置
# Enterprise Edition v3.0

version: '3.8'

services:
  # ============================================================
  # 网关服务
  # ============================================================
  gateway:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-gateway
    ports:
      - "9000:9000"
    environment:
      - LLM_HOST=0.0.0.0
      - LLM_PORT=9000
      - LLM_WORKER_URLS=http://vllm-worker-1:8000,http://vllm-worker-2:8000
      - LLM_ROUTING_STRATEGY=p2c_kv
      - LLM_RATE_LIMIT_ENABLED=true
      - LLM_RATE_LIMIT_REQUESTS_PER_SECOND=100
      - LLM_CIRCUIT_BREAKER_ENABLED=true
      - LLM_LOG_LEVEL=INFO
    depends_on:
      - vllm-worker-1
      - vllm-worker-2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - llm-network
    restart: unless-stopped

  # ============================================================
  # vLLM Worker 1 (示例)
  # ============================================================
  vllm-worker-1:
    image: vllm/vllm-openai:latest
    container_name: vllm-worker-1
    ports:
      - "8001:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model Qwen/Qwen2.5-0.5B-Instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --gpu-memory-utilization 0.9
    volumes:
      - huggingface-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - llm-network
    restart: unless-stopped

  # ============================================================
  # vLLM Worker 2 (示例)
  # ============================================================
  vllm-worker-2:
    image: vllm/vllm-openai:latest
    container_name: vllm-worker-2
    ports:
      - "8002:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=1
    command: >
      --model Qwen/Qwen2.5-0.5B-Instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --gpu-memory-utilization 0.9
    volumes:
      - huggingface-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - llm-network
    restart: unless-stopped

  # ============================================================
  # Prometheus (可选)
  # ============================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    networks:
      - llm-network
    restart: unless-stopped

  # ============================================================
  # Grafana (可选)
  # ============================================================
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    networks:
      - llm-network
    restart: unless-stopped

networks:
  llm-network:
    driver: bridge

volumes:
  huggingface-cache:
  prometheus-data:
  grafana-data:
